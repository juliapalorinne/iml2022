---
title: "IML Term project"
author: "Julia Palorinne, Pyry Silomaa, Sara Sippola"
output: pdf_document
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
```

```{r load data}
npfOrig <- read.csv("npf_train.csv")
```

```{r preprocess data}

# Dataset names:
# npfOrig          - Original data with has class2 as numeric values 0, 1
# npf          - "Original" data: has class2 with values 0,1 as numeric values, 
#                   columns removed: class4, partlybad, date (moved to rowname)
# npf.fact     - Otherwise the same as npf, but class2 is a factor with levels 0 and 1
# npf.168      - Processed data: Parameters that have different measurement heights have been reduced to height 16.8m
#                   columns removed: Parameters with height that's not 16.8m
# npf.168.fact -  Otherwise the same as npf.168, but class2 is a factor
# npfsp        - Reduced set of explanatory variables, received through running VIF analysis and repeating variable with highest VIF until no variable has a VIF
#                 higher than 10
# npfsp.fact   - Otherwise the same as npfsp but class2 is a factor


# Create class2, 0 = nonevent, 1 = event
npfOrig$class2 <- ifelse(npfOrig$class4 == "nonevent", 0, 1)

# Add date as row name
rownames(npfOrig) <- npfOrig[, "date"]

# Select global and 16.8 m high values
npf.168 <- data.frame(npfOrig[, (5:6)], npfOrig[, (13:16)], npfOrig[, (27:30)], 
                  npfOrig[, (41:42)], npfOrig[, (53:54)], npfOrig[, (63:72)],
                  npfOrig[, (83:88)], npfOrig[, (99:105)])

# Remove columns id, date, class4, partlybad from npf
npf <- npfOrig[, -(1:4)]

# Create datasets where class2 is a factor
npf.fact <- npf
npf.fact$class2 <- factor(npf.fact$class2)
npf.168.fact <- npf.168
npf.168.fact$class2 <- factor(npf.168.fact$class2)


# Divide numerical training data into two for training & testing
set.seed(42)
idx <- sample(nrow(npf), nrow(npf)/2)
npf_train <- npf[idx, ]
npf_test <- npf[-idx, ]

```

```{r functions}
## root mean squared error
rmse <- function(yhat, y) sqrt(mean((y - yhat)**2))

## Classification accuracy
cacc <- function(pred, true) sum(as.integer(as.logical(pred == true)))/length(pred)

## Accuracy
accuracy <- function(pred, y) mean(ifelse(pred >= 0.5, 1, 0) == y)

## Perplexity
## Force probabilities to [0.995, 0.005]
perplexity <- function(pred, y) {
  pred <- ifelse(pred != 0 & pred != 1, pred, ifelse(pred == 0, 0.005, 0.995 ))
  exp(-mean(log(ifelse(y == 1, pred, 1 - pred)))) 
}

# Cross-validate
# Split n items into k folds of roughly equal size
kpart <- function(n, k) {
    rep_len(1:k, length.out = n)
}

# Cross-validation predictor (from week 1 problem 2)
cv <- function(
    formula, # variables to use
    data, # data
    model = lm, # model to train
    n = nrow(data), # rows in the matrix
    k = min(n, 10), # cross-validation folds
    split = kpart(n, k), # the split of n data items into k folds
    train = function(data) model(formula, data = data),
    pred = function(model, data) predict(model, newdata = data)) {
  yhat <- NULL
  for (i in 1:k) {
    mod <- train(data[split != i, ])
    if (is.null(yhat)) {
      yhat <- pred(mod, data)
    } else {
      yhat[split == i] <- pred(mod, data[split == i, ])
    }
  }
  return(yhat)
}

loocv <- function(
    formula, # Formula specifying which variables to use
    data, # Dataset
    model = lm, # Type of model to train (as a function)
    ## function to train a model on data
    train = function(data) model(formula, data = data),
    ## function to make predictions on the trained model
    pred = function(model, data) predict(model, newdata = data)) {
  yhat <- NULL
  for (i in 1:nrow(data)) {
    ## go through all stations, train on other stations, and make a prediction
    mod <- train(data[-i, ])
    if (is.null(yhat)) {
      ## initialise yhat to something of correct data type,
      yhat <- pred(mod, data)
    } else {
      yhat[i] <- pred(mod, data[i, ])
    }
  }
  yhat # finally, output cross-validation predictions
}

# Find cross-validation predictions, return probabilities and not the classification
cv.prob <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               n = nrow(data), # number of rows in the data matrix
               k = min(n, 10), # number of cross-validation folds
               split = kpart(n, k), # the split of n data items into k folds
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) 
                 predict(model, newdata = data, type = "raw")[,2]) {
    yhat <- NULL
    for (i in 1:k) {
        ## go through all folds, train on other folds, and make a prediction
        mod <- train(data[split != i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[split == i] <- pred(mod, data[split == i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}

loocv.prob <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) 
                 predict(model, newdata = data, type = "raw")[,2]) {
    yhat <- NULL
    for (i in 1:nrow(data)) {
        ## go through all stations, train on other stations, and make a prediction
        mod <- train(data[-i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[i] <- pred(mod, data[i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}
```


# Classifiers

### Tree-based methods
```{r tree, warning=FALSE, message=FALSE}
set.seed(68)
library(tree)
library(ISLR2)


idx1 <- sample(nrow(npf.fact), nrow(npf.fact)/2)
npf.fact.train <- npf.fact[idx1, ]
npf.fact.test <- npf.fact[-idx1, ]
idx2 <- sample(nrow(npf.168.fact), nrow(npf.168.fact)/2)
npf.168.fact.train <- npf.168.fact[idx2, ]
npf.168.fact.test <- npf.168.fact[-idx2, ]

npf.168.fact.train

tree.train.fact <- tree(class2 ~ ., npf.fact.train)
summary(tree.train.fact)
tree.train.168.fact <- tree(class2 ~ ., npf.168.fact.train)
summary(tree.train.168.fact)

par(mfrow = c(1, 2))
plot(tree.train.fact)
plot(tree.train.168.fact)

prune.train.fact <- prune.misclass(tree.train.fact, best = 8)
prune.train.168.fact <- prune.misclass(tree.train.168.fact, best = 8)
par(mfrow = c(1, 2))
plot(prune.train.fact)
plot(prune.train.168.fact)

tree.pred.fact <- predict(prune.train.fact, npf.fact.test)
tree.pred.168.fact <- predict(prune.train.168.fact, npf.168.fact.test)
LOOCV.tree = loocv(prune.train.fact, data = npf.fact.test)
LOOCV.168.tree = loocv(prune.train.168.fact, data = npf.168.fact.test)
CV.tree <- cv(prune.train.fact, data = npf.fact.test)
CV.168.tree = cv(prune.train.168.fact, data = npf.168.fact.test)

print(cbind(tree.pred.fact, npf.168.fact.test$class2))
print(npf.168.fact)

acc.tree <- data.frame(original = accuracy(tree.pred.fact, npf.fact.test$class2),
                           CV = accuracy(CV.tree, npf.fact.test$class2),
                           LOOCV = accuracy(LOOCV.tree, npf.fact.test$class2))
acc.tree.168 <- data.frame(original = accuracy(tree.pred.168.fact, npf.168.fact.test$class2),
                           CV = accuracy(CV.168.tree, npf.168.fact.test$class2),
                           LOOCV = accuracy(LOOCV.168.tree, npf.168.fact.test$class2))
per.tree <- data.frame(original = perplexity(tree.pred.fact, npf.fact.test$class2),
                           CV = perplexity(CV.tree, npf.fact.test$class2),
                           LOOCV = perplexity(LOOCV.tree, npf.fact.test$class2))
per.tree.168 <- data.frame(original = perplexity(tree.pred.168.fact, npf.168.fact.test$class2),
                           CV = perplexity(CV.168.tree, npf.168.fact.test$class2),
                           LOOCV = perplexity(LOOCV.168.tree, npf.168.fact.test$class2))


knitr::kable(cbind(rbind(acc.tree, acc.tree.168), rbind(per.tree, per.tree.168))) %>%
  add_header_above(c("Accuracy" = 3, "Perplexity" = 3)) %>%
  kable_styling(latex_options = "HOLD_position")
```

Random forest tested with original and 168 dataset, and accuracies and perplexities reported.
Why is mtry 12?

```{r randomForest, warning=FALSE, message=FALSE}
library(randomForest)
set.seed(1)

# Select training and test data
idx <- sample(nrow(npf.168), nrow(npf.168)/2)
npf_train <- npf[idx, ]
npf_test <- npf[-idx, ]
npf_train168 <- npf.168[idx, ]
npf_test168 <- npf.168[-idx, ]

npf.RF.mod <- randomForest(class2 ~ ., data = npf_train, mtry = 12, importance = TRUE)
npf.RF.pred <- predict(npf.RF.mod, newdata = npf_test)
npf.RF.CV <- cv(npf.RF.mod, data = npf_test)
npf.RF.LOOCV <- loocv(npf.RF.mod, data = npf_test)

npf168.RF.mod <- randomForest(class2 ~ ., data = npf_train168, mtry = 12, importance = TRUE)
npf168.RF.pred <- predict(npf168.RF.mod, newdata = npf_test168)
npf168.RF.CV <- cv(npf168.RF.mod, data = npf_test168)
npf168.RF.LOOCV <- loocv(npf168.RF.mod, data = npf_test168)

yhat.RF <- predict(npf.RF.mod, newdata = npf_test)
plot(yhat.RF, npf_test$class2)

accuracies <- data.frame(original = accuracy(npf.RF.pred, npf_test$class2),
                           CV = accuracy(npf.RF.CV, npf_test$class2),
                           LOOCV = 0)
accuracies.168 <- data.frame(original = accuracy(npf168.RF.pred, npf_test168$class2),
                           CV = accuracy(npf168.RF.CV, npf_test168$class2),
                           LOOCV = accuracy(npf168.RF.LOOCV, npf_test168$class2))
perplexities <- data.frame(original = perplexity(npf.RF.pred, npf_test$class2),
                           CV = perplexity(npf.RF.CV, npf_test$class2),
                           LOOCV = 0)
perplexities.168 <- data.frame(original = perplexity(npf168.RF.pred, npf_test168$class2),
                           CV = perplexity(npf168.RF.CV, npf_test168$class2),
                           LOOCV = perplexity(npf168.RF.LOOCV, npf_test168$class2))
table <- cbind(rbind(accuracies, accuracies.168), rbind(perplexities, perplexities.168))

knitr::kable(table, digits = 3) %>%
  add_header_above(c("Accuracy" = 3, "Perplexity" = 3)) %>%
  kable_styling(latex_options = "HOLD_position")

```


