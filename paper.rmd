---
title: "IML Term project paper"
author: "Julia Palorinne, Pyry Silomaa, Sara Sippola"
output: pdf_document
urlcolor: blue
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(kableExtra)
```

```{r load data, echo=FALSE}
npfOrig <- read.csv("npf_train.csv")
# Create class2
npfOrig$class2[npfOrig$class4 == "nonevent"] <- 0
npfOrig$class2[npfOrig$class4 != "nonevent"] <- 1
# Add date as row name
rownames(npfOrig) <- npfOrig[, "date"]
# Remove columns id, date, class4, partlybad
npfData <- npfOrig[, -(1:4)]
```

# Introduction

In this project we trained a classifier on a data set of atmospheric measurements. The task is to predict whether new particle formation (NPF) happens or not on a given day based on the atmospheric data. We ran several different kinds of models, assessed through accuracy and perplexity calculations which of these models performed the best, and in this report we discuss the process and the results of our analyses.

# Preprocessing of the data

## Initial data analysis

The goal of this assignment is to predict the behavior of a multinomially distributed variable "class4", which describes new particle formation events on specific observation days. The variable is treated as a multinomial variable divided into four separate groups: no new particle formation (referred to in the dataset as "nonevent"), very clear and strong particle formation events (referred to in the dataset as class "Ia"), other particle formation events where the growth and formation rate could be determined with a sufficiently good confidence level (referred to in the dataset as class "Ib") and particle formation events where growth and formation rates could either not be defined or there was a possibility that the assessed rates were not sufficiently accurate. For the most part, we are observing a simpler version of this variable instead in the form of a binomial variable, let's refer to that as "class2", which is simply divided into no new particle formation events happening and new particle formation events happening.

Our dataset includes 464 observations of this variable as well as corresponding observations to a collection of 103 other variables. These variables include 50 numeric variables which describe the mean measurement related to phenomenona such as carbon dioxide concentration, solar radiation and air temperature during a measurement day, 50 numeric variables which describe the corresponding standard deviations to these measurements for those same measurement days, an id listing which runs from 1 to 464, the date of each measurement as well as a logical variable describing whether the other measurements are partly bad. We also notice that some of the 50 types of measurements for which means and standard deviations are calculated are in fact related to the same phenomena but are simply measured at different points. For example, one of the measurements is the amount of carbon dioxide but this is measured at four different heights.

In order to get a preliminary idea of what we are dealing with, let's visualize some of the variables we're observing as well as calculate some very simple measurements of these variables. First, let's download the dataset, define the binomial variable and observe how many of the observation of "class4" fall into specific classes.

```{r}
npf<-read.csv("npf_train.csv")
npf$class2<-npf$class4!="nonevent"
print(paste0("Relative amount of observations in class nonevent: ",mean(npf$class4=="nonevent")))
print(paste0("Relative amount of observations in class Ia: ",mean(npf$class4=="Ia")))
print(paste0("Relative amount of observations in class Ib: ",mean(npf$class4=="Ib")))
print(paste0("Relative amount of observations in class II: ",mean(npf$class4=="II")))
```

We observe that half of our observations are days with no new particle formation events, implying that half our observations are days with new particle formation events. Out of the classes related to days with new particle formation events, class I type events are more common than class II type events but class II type events are more common than either class Ia or class Ib type events. And within class type I events, we have more observations of class Ib type events than class Ia type events.

Let's also calculate basic summary statistics for the other measurements

```{r}
summary(npf)
```

Of particular interest here is that the variable "partlybad" seems to have only observations of type "FALSE". As a result, we can remove it from the dataset as it gives no extra information. We can also remove the variable id, as the order of the observations is not of interest in our analysis. And on top of this, it is unlikely that we will use the specific dates in relation to the observations either, so we will simply shift that information to be the rownames for the data.

```{r}
rownames(npf)<-npf[,2]
npf<-npf[,-c(1,2,4)]
```

Now we are left with our variable of interest, as well as 100 explanatory variables.

Let's next visualize some our explanatory variables via boxplots and scatterplots to get a deeper understanding of them. For the sake of simplicity, these visualizations grouped by the phenomena being measured. In order to also prelminarily look at how these variables behave in relation to new particle formation events, we'll also look at said measurements behave in relation to variable "class4". In terms of the boxplots, the boxplots are divided into groups by variable class4, and in terms of the scatterplots, the following colors represent different classes: red represents nonevent days, blue represents days with type Ia NPF events, green represents days with type Ib NPF events and black represents days with type II NPF events:

```{r}
cols<-c()
for(i in 1:464){
  if(npf$class4[i]=="nonevent"){
    cols<-c(cols,"red")
  } else if (npf$class4[i]=="Ia"){
    cols<-c(cols,"blue")
  } else if (npf$class4[i]=="Ib"){
    cols<-c(cols,"green")
  } else {
    cols<-c(cols, "black")
  }
}
par(mfrow=c(2,4))
for(i in 2:9){
plot(1:464,npf[,i],col=cols,main = colnames(npf)[i])
}
for(i in 2:9){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}

```
(Other visualizations omitted from the preliminary report.)
```{r, eval=FALSE}
cols<-c()
for(i in 1:464){
  if(npf$class4[i]=="nonevent"){
    cols<-c(cols,"red")
  } else if (npf$class4[i]=="Ia"){
    cols<-c(cols,"blue")
  } else if (npf$class4[i]=="Ib"){
    cols<-c(cols,"green")
  } else {
    cols<-c(cols, "black")
  }
}
par(mfrow=c(2,4))
for(i in 2:9){
plot(1:464,npf[,i],col=cols,main = colnames(npf)[i])
}
for(i in 2:9){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(1,2))
for(i in 10:11){
plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 10:11){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 12:23){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 12:23){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(1,2))
for(i in 24:25){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 24:25){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 26:37){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 26:37){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 38:49){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 38:49){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,5))
for(i in 50:59){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 50:59){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,4))
for(i in 60:67){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 60:67){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 68:79){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 68:79){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,2))
for(i in 80:85){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 80:85){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,5))
for(i in 86:95){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 86:95){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,2))
for(i in 96:99){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 96:99){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(1,2))
for(i in 100:101){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 100:101){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
```

In particular we notice that with several of the variables the boxplots for the nonevent group observations differ more from the observations for days with NPF events than the boxplots for the different NPF event groups both in terms of smaller or higher values but also spanning larger intervals (although this can partially be caused by there being observations within this group). Another notable thing is that the measurements of the same variables at different heights behave very similarly, which imply correlation between them (we take a more detailed look into this in the section *Correlation between parameters*). We also notice a few interesting sets of variables, such as nitrogen monoxide (measurements NO), potential temperature gradient (measurements PTG), rain indicator signal (measurements SWS), sulphur dioxide concentration (measurements SO2) and (measurements CS), which seem to mostly have very small standard deviations. This would imply that the shifts during the day in relation to the corresponding mean observations are on average very small.

## Correlation

### Correlation between predictors and class

Let's also observe correlations between our explanatory variables by themselves and the variable we're attempting to predict. As there is no natural way of turning the four-class multinomial variable into a numeric form (because the different classes don't have a clear direction in which they rise or fall), let's observe how the explanatory variables correlate with the variable "class" instead. This should help us get a preliminary sense of what explanatory variables might describe our variable of interest the best.

```{r}
cor(npf[,c(2:102)])[101,]
```

Based on this, we might as least preliminarily be interested in measurements of solar radiation (measurements Glob), net radiation (measurements Net), $O^3$ (measurements O3),  potential temperature gradient in C/m (measurements PTG), reflected solar radiation (measurements RGlob), temperature (measurements T)m type-A UV radiation (measurements UV_A) and measurements RHIRGA (which perhaps refer to some kind of relative humidity) as explanatory variables.

### Correlation between parameters

The correlation between the predictors and the value to be predicted is a very natural starting point when it comes to correlation analysis. It is also possible that the predictors themselves are correlated. Many classifiers are affected by high correlation, i.e. *collinearity* between variables. In regression models, for example, it can be difficult to differentiate between the individual effects of collinear variables on the response.

```{r original correlation plot, echo=FALSE, fig.dim=c(7, 7), message=FALSE}
library(corrplot)
# Original correlations
cmData <- cor(npfData)
corrplot(cmData, order = "FPC", tl.cex = 0.3, tl.col = "black")
``` 

As expected, the correlation plot shows that many of the variables describing the same phenomenon at different heights are correlated, as are variables related to radiation. For many classifiers, highly correlated parameters are problematic, and for this reason we would like to drop as many of these columns from the dataset as possible. 

We know that many of the parameters are measurements of the same thing at different heights. For example, water vapour concentration (H20) has been measured at heights 4.2m, 8.4m, 16.8m, 33.6m, 50.4m and 67.2m. As can be seen from the table below, the correlation coefficients between the daily means of water vapour concentration at different heights are essentially 1.

```{r H20 correlations, echo=FALSE, fig.dim=c(7, 7), message=FALSE}
kable(cmData[startsWith(colnames(cmData), "H2O") & endsWith(colnames(cmData), ".mean"), 
             startsWith(colnames(cmData), "H2O")  & endsWith(colnames(cmData), ".mean")],
      caption = "Correlation (H20)") %>%
  kable_styling(latex_options = "HOLD_position")
``` 

Hence we discard most of the measurements of H2O and other variables for which we have multiple highly correlating measurements, both the means and the standard deviations. We keep the parameters measured at 16.8 meters as for some parameters, the measurements have only been measured at that height. This rather crude method of discarding data leaves us with 38 unique columns. The figure below describes the correlation coefficients between the remaining parameters.

```{r correlation plot 168, echo=FALSE, fig.dim=c(6, 6)}
# Select global and 16.8 m high values for correlation plot
npf168 <- data.frame(npfOrig[, (5:6)], npfOrig[, (13:16)], npfOrig[, (27:30)], 
                  npfOrig[, (41:42)], npfOrig[, (53:54)], npfOrig[, (63:72)],
                  npfOrig[, (83:88)], npfOrig[, (99:105)])
cm168 <- cor(npf168)
corrplot(cm168, order = "FPC", tl.cex = 0.5, tl.col = "black")
```

The correlation plot for the remaining parameter shows us that the remaining highly correlated parameters are all radiation-related.

# Classifier

## Description of considered machine learning approaches
In order to use a model that describes the process that creates the data the best, we attempted to use several different kinds of models to assess the data. After defining each of these models, we would assess which of the models we used performed best and use said model in our predictions.

Specifically, the two measures we used to assess the quality of each model were its accuracy and perplexity. We defined accuracy as simply the relative amount of values the model predicted correctly, expressed mathematically for variables of interest $y_i$ and related predictions $\hat{y}_i$ as $\frac{\sum_{i=1}^{n}I(y_i=\hat{y}_i)}{n}$. This measures whether the model defines the labels correctly. All values naturally receieve values within interval $[0,1]$ and higher values imply better results.

Perplexity instead expresses the relative confidence in those predictions. The models we use give estimated probabilities $\hat{p}(\hat{y_i}=y)$ that the datapoints $\hat{y_i}$ we're predicting receive specific values $y$. Thus, if the model is descriptive of the phenomena which create the dataset, it would seem reasonable that for the actual realized values $y$ the measure $\hat{p}(\hat{y_i}=y)$ would be relatively high. Thus its logarithmic transformation $\log(\hat{p}(\hat{y_i}=y))$ would also be able to receive high values, and similarly $-\log(\hat{p}(\hat{y_i}=y))$ and $e^{\log(\hat{p}(\hat{y_i}=y))}$ would receive small values. Thus we can use as a corresponding overall measure for the dataset the measurement $e^{-\frac{\sum_{i=1}^n\log(\hat{p}(\hat{y_i}=y_{i}))}{n}}$ where $y_i$ are realized values of the observations we're attempting to assess through $\hat{y_i}$. As $\hat{p}(y_i=y)\in [0,1]$ for all $\log(\hat{p}(y_i=y)) \le 0$ for all y. Thus, all values of our general perplexity measure are within interval $[1,\infty)$ and smaller values are preferred.

However, it is not sufficient to simply define the model on the training dataset and assess accuracy and perplexity on said training dataset as this would woefully inflate the accuracy assessment and deflate the perplexity assessment of the model on a general dataset. We instead attempted to assess accuracy and perplexity through cross-validation methods, specifically leave-one-out cross-validation (loo-cv). The idea behind loo-cv is that for each observation in the training dataset, a model is trained based on all other observations in the dataset. This new model is then used to predict the training dataset value that was left out of defining the model. With this method, we receive based on observations of explanatory variables both predicted values and estimates of probabilities for specific events for each measurement in a way that didn't include the observations in both defining the model training and assessing its quality. Thus, we can use these measures in our accuracy and perplexity calculations to better assess how the model using all the observations in the training dataset behaves on a completely separate test dataset.
```{r preprocess data, echo=FALSE}
# Sample half of the data for training and testing sets
set.seed(42)
idx <- sample(nrow(npfData), nrow(npfData)/2)
npf_train <- npfData[idx, ]
npf_test <- npfData[-idx, ]
```
Next, let's discuss some of the models we considered

### Dummy model

As our first model, we simply attempted to predict the values through a dummy model, which predicts all values into the more common of the two classes. However, as half of the observations are nonevent days and half of the observations are event days, accuracy would only be 50% no matter which the model would choose. As a result, the dummy model is not sufficient to use in this problem

### Logistic regression

Logistic regression models can be used for binomial variables as well as for multinomial variables under specific assumptions. The idea of logistic regression in a binary case is that our variable of interest $Y$ is binomially distributed with parameter $q\in [0,1]$ representing probability that $Y$ receieves one of the values. Now, with explanatory variables $X$ we assume that these $X$ influence the parameter $p$ for each observation, meaning $p(Y|X)=q(X)^{Y}(1-q(X))^{1-Y}$ is a binomial probability with parameter $q(X)$ being of form $\sigma(\sum \lambda_i X_i)$ for values of individual explanatory variables $X_i$ and coefficients $\lambda_i$ and link function $\sigma: \mathbb{R}\to [0,1]$. Thus we can estimate a logistic regression by finding parameters $\lambda_i$ that maximize the likelihood function $\prod p(Y_i|X)$.

Notable benefits of the logistic regression model include that as a discriminative model it eventually reaches a lower asymptotic variance than comparable generative models. On top of this, compared to for example comparable support vector classifiers, on average logistic regression models fare better under situations where the classes are not easily separable by explanatory variables.

However, possible shortcomings of the logistic regression model include assumptions of linearity in relation to the explanatory variables and the parameter (which we could attempt to correct but with such a large collection of explanatory variables, this at least can't be applied immediately), assumptions of for example link function that are required to define coefficients, and especially with high-dimensional models, multicollinearity can become a notable problem.

In our model, we used specifically the logistic link function, implying $\sigma(x)=\frac{e^{x}}{e^{x}+1}$, although we acknowledge that other choices such as the probit link could have also been viable options. The estimation of the model was done with base r function glm. In order to curb issues related to multicollinearity, we first limited ourselves to the dataset with observations measured at multiples only observed at 16.8 meters. After this, we reduced the set of explanatory variables further via variance inflation factors.

Variance inflation factors (VIFs) can be calculated for each explanatory variable in a model through assessing linear models for each of the explanatory variables. The VIF for explanatory variable $X_i$ is defined as $\frac{1}{1-R_i^2}$ where $R_i^2$ is the amount of variance of $X_i$ explained through a linear model by the other explanatory variables. Thus, these values express how much the explanatory variable in question can be described by a linear combination of the other explanatory variables. Thus, particularly high VIFs imply multicollinearity being present in the model. We calculated VIFs for our model with function vif from R package car.

As removing one explanatory variable effects the VIFs of all other variables, we chose to implement an iterative process to remove variables based on VIFs. After defining a logistic regression model with all explanatory variables, we calculated the VIFs, assessed which variable had the highest VIF, removed both this variable and the other variable describing the same set of observations (if a mean variable had the highest VIF, we would also remove the corresponding standard deviation and if a standard deviation variable had the highest VIF, we would also remove the corresponding mean variable). We decided to do this as it felt imprudent to choose to not include one descriptor of an observation in the baseline model which we're assessing while including the other one. After this, we would repeat the process until the highest VIF value for a variable was less than 10. The choice of the limit was based on descriptions in the course material.

After this, we had a remaining set of 18 explanatory variables, which we modelled using logistic regression. After checking the coefficients, there were still variables for which the likelihood ratio tests gave relatively high p-values for hypotheses involving non-zero coefficients. In order to deal with this, we set those coefficients equal to zero and assessed the model with the remaining explanatory variables. Through this, we received a simpler model with  explanatory variables.

### Lasso regression

Lasso regression has obvious links to logistic regression but it includes a penalty term which punishes models with high values of coefficients. In comparable linear regression minimizing squared error, the minimizable function is the sum of the squared error terms and $\sum_{i=1}^n \gamma |\lambda_i|$ where $\gamma>0$. We similarly as in logistic regression assess coefficients for a model that describes parameter $q$ of a binomial model that minimizes this function.

One of the benefits of lasso regression is that including this penalty term in the optimization process leads to simpler models with fewer explanatory variables, as also lower-dimensional models lead are preferred. Thus, the lasso model in and of itself can remove variables which are highly correlated with other variables from the model and thus at least alleviate multicollinarity within the model. However, the selection between possible correlated variables can apparently be rather random and thus not perhaps fully descriptive of the data-generating process.

In terms of our modelling, we decided to use lasso instead of a comparable method called ridge regression, which uses penalty term $\sum_{i=1}^n \gamma \lambda_i^2$. The reason for this is that we preferred to make a simpler model with fewer explanatory variables rather than ridge regression, which tends to not reduce the dimension of the model but rather simply give similar coefficients to all correlated explanatory variables. In order to estimate the model, we used R functions glmnet and cv.glmnet from package glmnet. We attempted to assess the value of $\gamma$ which would minimize loo cross-validated misclassification rate with the function cv.glmnet. After defining this value, we calculated cross-validated accuracy (which should be equivalent to what was calculated by cv.glmnet) as well as the cross-validated perplexity for the model estimated by glmnet.

## Generative models

In logistic regression models, we directly model $p(Y|X)$, i.e. the conditional distribution of the response $Y$ given the predictors $X$. Generative models offer a less direct approach, where the focus is in modeling $p(X|Y)$ and using these estimates to estimate $p(Y|X)$ for each possible class $k$ with Bayes' theorem:
\begin{equation*}
p(Y=k|X) = \frac{\pi_k p(X|Y=k)}{\sum_i \pi_i p(X|Y=i)}.
\end{equation*}
Here $pi_k$ is an estimate of the prior probability that a random observation comes from the $k$th class, in our case computed as the fraction of the observations in the training dataset that are in the $k$th class. 

While the general approach is the same for all generative models, they differ in how they estimate $p(X|Y)$. In a dataset with $p$ predictors, estimating $p(X|Y)$ amounts to estimating a $p$-dimensional density function for an observation in the $k$th class. The task is challenging, as we must consider the distribution of each predictor on its own *and* the joint distribution of the predictors. Different models make different assumptions that mitigate the difficulty.

### Linear Discriminant Analysis

A linear discriminant analysis (LDA) classifier assumes that all $p$ predictors $X = (X_1, \dots, X_p)$ are drawn from a multivariate Gaussian distribution. This means that each predictor follows a normal distribution $N(\mu_k, \Sigma)$ where $\mu_k$ is the class-specific mean and $\operatorname{Cov}(X) = \Sigma$ the covariance matrix of $X$, and there is some correlation between each pair of predictors. The estimate of $p(X|Y)$ is
\begin{equation*}
p(X|Y) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \right)
\end{equation*}
meaning that the classifier assigns an observation $X$ to to the class for which
\begin{equation*}
\delta_k(x) = x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
\end{equation*}
is maximized. 
The covariance matrix being the same for all classes is a defining feature of linear discriminant analysis. The assumption makes the model linear and reduces the number of parameters to estimate. This reduces the flexibility and hence lowers the variance of the model. Lower variance can mean that the model performs well, but if the assumption is not reflected in the data, bias can be high and overall performance suffers due to the bias-variance trade-off.

For linear discriminant analysis to work, it is important to have enough observations with regards to the number of predictors, as performance suffers greatly as the number of predictors approaches the number of observations.

In our modeling, we used the function *lda* from the library *MASS*. We tested the performance of the model on the dataset with observations measured at multiples only observed at 16.8 meters.

### Quadratic Disriminant Analysis 

The assumption that the covariance matrix is shared for all classes is quite strict. Allowing each class to have it's own covariance matrix brings us to quadratic discriminant analysis (QDA). The observations are still assumed to be drawn from a multivariate Gaussian distrbution with a class-specific mean vector, so otherwise the assumptions remain the same as in LDA. Each predictor now follows a normal distribution $N(\mu_k, \Sigma_k)$. The expression to be maximized is 
\begin{align*}
\delta_k(x) = x^T\Sigma_k^{-1}\mu_k-\frac{1}{2}x^T\Sigma_k^{-1}x-\frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k -\frac{1}{2} \log |\Sigma_k| + \log \pi_k.
\end{align*}
Generally, quadratic discriminant analysis is expected to perform better that linear discriminant analysis on large data sets, but as in linear discriminant analysis, the number of predictors must be small enough to produce decent results. Quadratic discriminant analysis is also recommended when the assumption that all classes share a covariance matrix is clearly unfounded.

In our approach, we used the *qda* function from the *MASS* library, and as with linear discriminant analysis, tested the performance of the model on both the original dataset and the dataset with observations measured at multiples only observed at 16.8 meters.

### Naive Bayes

A naive Bayes classifier makes no assumptions on the distribution of the observations, and instead assumes that within the *k*th class, the *p* predictors are independent. In other words, the assumption is that there is no association between the predictors, and therefore no joint distribution to consider. With this, the posterior probability can be computed as
\begin{equation*}
p(Y=k|X) = \frac{\pi_k p(X_1|Y=k) \cdots p(X_p|Y=k)}{\sum_i \pi_i p(X_1|Y=i) \cdots p(X_p|Y=i)}.
\end{equation*}
In a naive Bayes model, there are a few possible ways to estimate $p(X|Y)$. For quantitative predictors such as ours, we assume that within each class, the *j*th predictor is drawn from a univariate normal distribution, i.e. $X_j|Y = k \approx N (\mu_{jk}, \sigma^2_jk)$.
The assumption of independence between predictors is often not realistic, but the model can perform well nonetheless. This is especially true for small datasets, since estimating a joint distribution requires a large amount of data. 


### Support Vector Classifiers

The idea of the binary support vector classifier is based on the notion of finding within a $p$-dimensional problem a $(p-1)$-dimensional hyperplane $\lambda_0+\sum\lambda_i x_i=0$ to divide the set of explanatory variables. Now, for this hyperplane, we can divide our observations into two groups, those values for which $\lambda_0+\sum_{i=1}^m\lambda_j x_i>0$ and those values for which $\lambda_0+\sum\lambda_i x_i<0$. Thus, this can function as a model through which we can predict binary variables such as our variable of interest.

However, defining such hyperplanes can be slightly difficult as all values might not be separable by a hyperplane or the hyperplane that separates values can overfit to training data. Thus, we define a soft-margin classifier, which has the benefits of letting some values be misclassified or be closer to the hyperplane than a strict maximal margin classifier would define, which leads to a more robust model that fits most of the training dataset better. 

In order to define this, we need to find the optimal hyperplane to separate the observations. This optimization problem is an optimization problem of $M$ in relation to coefficients $\lambda_i$ and slack variables $\epsilon$ within inequation $y_i(\lambda_0+\sum_{j=1}^m\lambda_jx_{ij})\ge M(1-\epsilon_i)$ with limitations $\sum_{j=1}^{m}\lambda_j^2=1$, $\epsilon_i\ge0$ and $\sum_{i=1}^n\epsilon_i\le C$ for tuning parameter $C$.

As in our optimization the data only depends on the inner products of the explanatory variables and the values of the variable of interest, we can use different generalizations of inner products to gain different kinds of models. Thus with different choices of kernels we can model different kinds of data.

Other benefits benefits of using support vector classifiers include that they have been proven to behave well in several different applications and that it has been shown that on average support vector classifiers perform better than logistic regression models in problems where the classes are well-separated. However, the model can vary quite notably based on the tuning parameter $C$ as this parameter affects the amount of observations that violate the margin and thus increases or decreases the amount of support vectors that influence the model.

For our model, we used functions tune and svm from R package e1071. We chose to run a 10-fold cross validation on a set of tuning parameters $C$ to assess which kernel performed best and chose to model the value based on that. In these preliminary assessments, we assessed the radial and linear kernels had the best performance of the ones available in package e1071. For these kernels, we ran loo-cross validation to assess the value of tuning parameter $C$ and calculated for said tuning parameter loo-cross-validated accuracy and perplexity. Eventually, the radial kernel had slightly better accuracy but the linear kernel had lower perplexity and as a result, we chose to use the linear kernel in our model.

### Classification trees

In tree-based methods, the data points are divided into smaller and smaller fragments based on some set of rules. A new division happens at each branch, so when the size of the tree grows, it becomes more and more fitted to the data. When fitting classification trees, the process is often first started with a very large tree, and then the unnecessary branches are removed in the process of pruning, until the complexity of the tree is reduces to the optimal level. Constructing the trees, pruning, and evaluating the results can be done in a variety of different ways.

A often used tree-based classification method is called Random Forest. It is based on the process described above, but only a random subset of the features is considered at each split of the tree. This is repeated multiple times and in the end, the obtained forest is evaluated and the best splits at each point are found. For Random Forest classifier, we used the R library *randomForest*.


## Classifier performances (numerical)

In order to reduce knitting time for the file, we will in this preliminary report instead of expressing the full R code to calculate the values, simply report the results we received and instead return the full R code only with the final report. The results for the two-class models in terms of loo-cross-validated accuracy and perplexity are:
\begin{table}
\begin{tabular}{|r|cc|}
\hline
Model type & loo-cv accuracy & loo-cv perplexity\\
\hline
Dummy model & 0 & 2.01\\
Logistic regression & 0.89 & 1.30\\
Naive Bayes & 0.81 & 152.85\\
LDA & 0.89 & 1.33\\
QDA & 0.85 & 6.34\\
SVM & 0.88 & 1.34\\
Lasso & 0.89 & 1.29\\
Random Forest & 0.89 & 1.35\\
\hline
\end{tabular}
\end{table}

## Chosen classifier, pros and cons of this particular classifier for this application

With our observed best-performing models being the logistic regression model, the random forest model, the Lasso regression model and the linear discriminant analysis. Out of these models, we eventually chose to use the logistic regression model. The Lasso model was our best-performing model, but we eventually chose not to use it for practical reasons related to computational heaviness in calculating the multi-class model. Mainly, the amount of information we could gain through calculations in R without R crashing was relatively limited, to the degree that we couldn't always even calculate the cross-validated best choice for the tuning parameter.

We eventually also chose to not use the LDA model after assessing the assumptions related to the model. LDA models have been found to perform best with a limited amount of variables and we still question whether we should attempt to reduce the amount of variables from the model further from the current amount of 38 from the model. On top of this, considering the assumption of equal variances, the boxplots we drew earlier in the report imply that class "nonevent" might have larger variance for some variables than class "event".

We made the decision of model based on preliminary results, and though classification trees would represent an equally viable solution, we calculated those results only later on, and by that point had already calculated the results for the logistic regression model. However, for the final report we might consider transitioning to the classification tree model.

Thus we were left with the logistic regression model, which was also lower-dimensional than many of the other models with only 10 explanatory variables. However, we could question in relation to this model whether there is sufficient basis to for example remove certain variables, which might describe relevant information, as well as for example whether including an intercept in the model would be meaningful in terms of interpretation. Currently, several of these decisions are based on likelihood-ratio tests performed within the R function glm, which was pursued in an attempt to create a lower-dimensional model. Also, comparing the logistic regression model to for example the Lasso, which does variable selection on a model with a somewhat comparable minimzable loss function.

## Multiclass-classifier
we will extend it into a multinomial logistic regression model in order to assess the multi-class accuracy for our model. The basic idea of multinomial logistic regression here is that out of $K$ classes we choose a baseline value of our variable of interest, and compare other values to it, leading to $p(Y=k|X=x)=\frac{e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}{1+\sum_{k=1}^{K-1} e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}$ where our baseline class is class $K$. Alternatively, we can use equivalent so-called softmax coding where we  treat all classes equivalently and thus $p(Y=k|X=x)=\frac{e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}{\sum_{k=1}^K-1 e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}$. The interpretation of logistic regression coefficients comes now from the following: $\log(\frac{p(\hat{y}_i=k_a)}{p(\hat{y_i}=k_b)})=(\lambda_{a0}-\lambda_{b0})+\sum_{j=1}^n(\lambda_{aj}-\lambda_{bj})x_{ij}$. Thus the interpretation of these coefficients is in relation to the difference of the coefficients for different classes and describe the ratio between the probabilities that our variable of interest receives a value in one of these two classes.

Similarly as in logistic regression, we attempt to define coefficients $\lambda_j$ in a way that maximizes likelihood.

In our model, we used the same dataset as in our binary logistic regression model, in order to avoid issues of multicollinearity. In order to define this model, we used function multinom from R package nnet with softmax codin. This creates estimated probabilities $\hat{p}(\hat{y}_i=y)$ for all four classes $y$, and through that we could calculate loo-cross-validated values for accuracy as well as perplexity.

Eventually, we chose to emphasize class2 accuracy, and as a result our final predictions are based on estimating models for both class2 and class4. We first used our model for class2 to predict whether a value would be an event or not and for values the model assessing class2 predicted to be event-days, we referred to our class4 model to estimate which of the event classes had the highest probability.

# Results

We notice in terms of multiclass accuracy that while the model predicts in particular nonevents rather well, the loo-cross-validated predictions are less successful for calculating distinct event types. For example, less than 10% of days which belonged to class Ia were predicted into this class. As a result, further changes should be made to the model in the future to make these predictions more viable.

## Insights, conclusions, discussion etc.
