---
title: "IML Term project paper"
author: "Julia Palorinne, Pyry Silomaa, Sara Sippola"
output: pdf_document
bibliography: IML.bib
csl: apa.csl
urlcolor: blue
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(kableExtra)
```

```{r load data, echo=FALSE}
npfOrig <- read.csv("npf_train.csv")
# Create class2
npfOrig$class2[npfOrig$class4 == "nonevent"] <- 0
npfOrig$class2[npfOrig$class4 != "nonevent"] <- 1
# Add date as row name
rownames(npfOrig) <- npfOrig[, "date"]
# Remove columns id, date, class4, partlybad
npfData <- npfOrig[, -(1:4)]
```

# Introduction

In this project we trained a classifier on a data set of atmospheric measurements. The task is to predict whether new particle formation (NPF) happens or not on a given day based on the atmospheric data. We ran several different kinds of models, assessed through accuracy and perplexity calculations which of these models performed the best, and in this report we discuss the process and the results of our analyses. Our main reference throughout the report is the course book [@book], and if a reference is not specified when discussing machine learning theory, the reader can safely assume that we rely on [@book].

# Preprocessing of the data

## Initial data analysis

The goal of this assignment is to predict the behavior of a multinomially distributed variable "class4", which describes new particle formation events on specific observation days. The variable is treated as a multinomial variable divided into four separate groups which are according to the article "Formation and growth of fresh atmospheric aerosols: eight years of aerosol size distribution data from SMEAR II, Hyytiälä, Finland" [@eventstat]: no new particle formation (referred to in the dataset as "nonevent"), very clear and strong particle formation events (referred to in the dataset as class "Ia"), other particle formation events where the growth and formation rate could be determined with a sufficiently good confidence level (referred to in the dataset as class "Ib") and particle formation events where growth and formation rates could either not be defined or there was a possibility that the assessed rates were not sufficiently accurate. For the most part, we are observing a simpler version of this variable instead in the form of a binomial variable, let's refer to that as "class2", which is simply divided into no new particle formation events happening and new particle formation events happening.

Our dataset includes 464 observations of this variable as well as corresponding observations to a collection of 103 other variables. These variables include 50 numeric variables which describe the mean measurement related to phenomena such as carbon dioxide concentration, solar radiation and air temperature during a measurement day, 50 numeric variables which describe the corresponding standard deviations to these measurements for those same measurement days, an id listing which runs from 1 to 464, the date of each measurement as well as a logical variable describing whether the other measurements are partly bad. We also notice that some of the 50 types of measurements for which means and standard deviations are calculated are in fact related to the same phenomena but are simply measured at different points. For example, one of the measurements is the amount of carbon dioxide but this is measured at four different heights.

In order to get a preliminary idea of what we are dealing with, let's visualize some of the variables we're observing as well as calculate some very simple measurements of these variables. First, let's download the dataset, define the binomial variable and observe how many of the observation of "class4" fall into specific classes.

```{r}
npf<-read.csv("npf_train.csv")
npf$class2<-npf$class4!="nonevent"
print(paste0("Relative amount of observations in class nonevent: ",mean(npf$class4=="nonevent")))
print(paste0("Relative amount of observations in class Ia: ",mean(npf$class4=="Ia")))
print(paste0("Relative amount of observations in class Ib: ",mean(npf$class4=="Ib")))
print(paste0("Relative amount of observations in class II: ",mean(npf$class4=="II")))
```

We observe that half of our observations are days with no new particle formation events, implying that half our observations are days with new particle formation events. Out of the classes related to days with new particle formation events, class I type events are more common than class II type events but class II type events are more common than either class Ia or class Ib type events. And within class type I events, we have more observations of class Ib type events than class Ia type events.

Let's also calculate basic summary statistics for the first few variables. The full summary of the whole dataset is available in appendix A.

```{r}
summary(npf[,1:4])
```

Of particular interest here is that the variable "partlybad" seems to have only observations of type "FALSE". As a result, we can remove it from the dataset as it gives no extra information. We can also remove the variable id, as the order of the observations is not of interest in our analysis. And on top of this, it is unlikely that we will use the specific dates in relation to the observations either, so we will simply shift that information to be the rownames for the data.

```{r}
rownames(npf)<-npf[,2]
npf<-npf[,-c(1,2,4)]
```

Now we are left with our variable of interest, as well as 100 explanatory variables.

The key objective in this project, classifying NPF event and nonevent days correctly, is an example of *supervised learning*: our data set contains the correct classes, and we can use these in the training of our models. With *unsupervised learning* we can look for insights into the data without using the known classes. There are many different types of unsupervised learning, discussed in [@book, p.497-552], but since our goal is classification, we focus on *cluster analysis*. This means that we look for subgroups of similar observations. In particular, we use K-means clustering to see if we can easily differentiate between event and nonevent days, and between event types Ia, Ib, II and nonevent, without using the given classes except to validate the results. The K-means clustering algorithm starts by assigning each observation to a cluster at random. At every iteration, the algorithm computes a *cluster centroid*, a vector of the variable means for each observation in the *k*th cluster, and then reassigns each observation to the cluster with the nearest centroid. The nearest centroid is the one to which an observation has the smallest Euclidean distance. 

Recall that we have daily means and standard deviations in the dataset. For the purposes of K-means clustering, we restrict ourselves to the means, and because the variables are measured on very different scales, we normalize them to have zero mean and unit variance with the *scale* function. The random cluster assignments tend to have an effect on the end result, so we run the *kmeans* function with 1000 random starting points, and look at the best result. With 2 clusters, the confusion matrix given by the *solve_LSAP* function from the [@clue] package that matches the clusters with the known classes, is as follows:

```{r}
## choose variables whose names end with ".mean" together with "class4"
vars <- colnames(npf)[sapply(
  colnames(npf),
  function(s) nchar(s) > 5 && substr(s, nchar(s) - 4, nchar(s)) == ".mean"
)]
npf.means <- npf[, c(vars, "class2","class4")]
npf.means$class2 <- factor("event", levels = c("nonevent", "event"))
npf.means$class2[npf$class4 == "nonevent"] <- "nonevent"

## strip the trailing ".mean" to make the variable names prettier
colnames(npf.means)[1:length(vars)] <- sapply(
  colnames(npf.means)[1:length(vars)],
  function(s) substr(s, 1, nchar(s) - 5)
)
vars <- colnames(npf.means)[1:length(vars)]

set.seed(42)

library(clue)

cl <- kmeans(scale(npf.means[, vars]),
  centers = 2, algorithm = "Lloyd", nstart = 1000, iter.max = 100
)

## Create a confusion matrix between the known classes (class 4) and
## cluster indices.
tt <- table(npf.means$class2, cl$cluster)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt <- tt[, solve_LSAP(tt, maximum = TRUE)]

print(tt)

```
K-means clustering assigns 178 observations to cluster 1 and 286 to cluster 2, so it does not quite catch the 50-50 division in the dataset. It has assigned 58% of nonevents to class 1, and 81% of events to class 2, which amounts to a total accuracy of 69% if we think of the clustering result as a classifier. With 4 clusters, the accuracy of our cluster-classifier drops down to 46%. The K-means algorithm cannot easily identify the correct event classes.

```{r}

cl <- kmeans(scale(npf.means[, vars]),
  centers = 4, algorithm = "Lloyd", nstart = 1000, iter.max = 100
)

## Create a confusion matrix between the known classes (class 4) and
## cluster indices.
tt <- table(npf.means$class4, cl$cluster)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt <- tt[, solve_LSAP(tt, maximum = TRUE)]

print(tt)

```


Let's next visualize with R [@RDCT2008] some our explanatory variables via boxplots and scatterplots to get a deeper understanding of them. For the sake of simplicity, these visualizations grouped by the phenomena being measured. In order to also preliminarily look at how these variables behave in relation to new particle formation events, we'll also look at said measurements behave in relation to variable "class4". In terms of the boxplots, the boxplots are divided into groups by variable class4, and in terms of the scatterplots, the following colors represent different classes: red represents nonevent days, blue represents days with type Ia NPF events, green represents days with type Ib NPF events and black represents days with type II NPF events:

```{r}
cols<-c()
for(i in 1:464){
  if(npf$class4[i]=="nonevent"){
    cols<-c(cols,"red")
  } else if (npf$class4[i]=="Ia"){
    cols<-c(cols,"blue")
  } else if (npf$class4[i]=="Ib"){
    cols<-c(cols,"green")
  } else {
    cols<-c(cols, "black")
  }
}
par(mfrow=c(2,4))
for(i in 2:9){
plot(1:464,npf[,i],col=cols,main = colnames(npf)[i])
}
for(i in 2:9){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}

```
(Other visualizations omitted from the preliminary report.)
```{r, eval=FALSE}
cols<-c()
for(i in 1:464){
  if(npf$class4[i]=="nonevent"){
    cols<-c(cols,"red")
  } else if (npf$class4[i]=="Ia"){
    cols<-c(cols,"blue")
  } else if (npf$class4[i]=="Ib"){
    cols<-c(cols,"green")
  } else {
    cols<-c(cols, "black")
  }
}
par(mfrow=c(2,4))
for(i in 2:9){
plot(1:464,npf[,i],col=cols,main = colnames(npf)[i])
}
for(i in 2:9){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(1,2))
for(i in 10:11){
plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 10:11){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 12:23){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 12:23){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(1,2))
for(i in 24:25){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 24:25){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 26:37){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 26:37){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 38:49){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 38:49){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,5))
for(i in 50:59){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 50:59){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,4))
for(i in 60:67){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 60:67){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,4))
for(i in 68:79){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 68:79){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(3,2))
for(i in 80:85){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 80:85){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,5))
for(i in 86:95){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 86:95){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(2,2))
for(i in 96:99){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 96:99){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
par(mfrow=c(1,2))
for(i in 100:101){
  plot(1:464,npf[,i],col=cols,main=colnames(npf)[i])
}
for(i in 100:101){
  boxplot(npf[,i]~npf$class4,main = colnames(npf)[i],names=c("Ia","Ib","II","N"))
}
```

In particular we notice that with several of the variables the boxplots for the nonevent group observations differ more from the observations for days with NPF events than the boxplots for the different NPF event groups both in terms of smaller or higher values but also spanning larger intervals (although this can partially be caused by there being observations within this group). Another notable thing is that the measurements of the same variables at different heights behave very similarly, which imply correlation between them (we take a more detailed look into this in the section *Correlation between parameters*). We also notice a few interesting sets of variables, such as nitrogen monoxide (measurements NO), potential temperature gradient (measurements PTG), rain indicator signal (measurements SWS), sulphur dioxide concentration (measurements SO2) and (measurements CS), which seem to mostly have very small standard deviations. This would imply that the shifts during the day in relation to the corresponding mean observations are on average very small.

## Correlation

### Correlation between predictors and class

Let's also observe correlations between our explanatory variables by themselves and the variable we're attempting to predict. As there is no natural way of turning the four-class multinomial variable into a numeric form (because the different classes don't have a clear direction in which they rise or fall), let's observe how the explanatory variables correlate with the variable "class" instead. This should help us get a preliminary sense of what explanatory variables might describe our variable of interest the best.

```{r}
cor(npf[,c(2:102)])[101,]
```

Based on this, we might as least preliminarily be interested in measurements of solar radiation (measurements Glob), net radiation (measurements Net), $O^3$ (measurements O3),  potential temperature gradient in C/m (measurements PTG), reflected solar radiation (measurements RGlob), temperature (measurements T)m type-A UV radiation (measurements UV_A) and measurements RHIRGA (which perhaps refer to some kind of relative humidity) as explanatory variables.

### Correlation between variables

The correlation between the predictors and the value to be predicted is a very natural starting point when it comes to correlation analysis. It is also possible that the predictors themselves are correlated. Many classifiers are affected by high correlation, i.e. *collinearity* between variables. In regression models, for example, it can be difficult to differentiate between the individual effects of collinear variables on the response. Let's visualize these correlations with R package [@corrplot]

```{r original correlation plot, echo=FALSE, fig.dim=c(7, 7), message=FALSE}
library(corrplot)
# Original correlations
cmData <- cor(npfData)
corrplot(cmData, order = "FPC", tl.cex = 0.3, tl.col = "black")
``` 

As expected, the correlation plot shows that many of the variables describing the same phenomenon at different heights are correlated, as are variables related to radiation. For many classifiers, highly correlated variables are problematic, and for this reason we would like to drop as many of these columns from the dataset as possible. 

We know that many of the variables are measurements of the same thing at different heights. For example, water vapour concentration (H20) has been measured at heights 4.2m, 8.4m, 16.8m, 33.6m, 50.4m and 67.2m. As can be seen from the table below, the correlation coefficients between the daily means of water vapour concentration at different heights are essentially 1.

```{r H20 correlations, echo=FALSE, fig.dim=c(7, 7), message=FALSE}
kable(cmData[startsWith(colnames(cmData), "H2O") & endsWith(colnames(cmData), ".mean"), 
             startsWith(colnames(cmData), "H2O")  & endsWith(colnames(cmData), ".mean")],
      caption = "Correlation (H20)") %>%
  kable_styling(latex_options = "HOLD_position")
``` 

Hence we discard most of the measurements of H2O and other variables for which we have multiple highly correlating measurements, both the means and the standard deviations. We keep the parameters measured at 16.8 meters as for some parameters, the measurements have only been measured at that height. This rather crude method of discarding data leaves us with 38 unique columns. The figure below describes the correlation coefficients between the remaining variables

```{r correlation plot 168, echo=FALSE, fig.dim=c(6, 6)}
# Select global and 16.8 m high values for correlation plot
npf168 <- data.frame(npfOrig[, (5:6)], npfOrig[, (13:16)], npfOrig[, (27:30)], 
                  npfOrig[, (41:42)], npfOrig[, (53:54)], npfOrig[, (63:72)],
                  npfOrig[, (83:88)], npfOrig[, (99:105)])
cm168 <- cor(npf168)
corrplot(cm168, order = "FPC", tl.cex = 0.5, tl.col = "black")
```

The correlation plot for the remaining variables shows us that the remaining highly correlated variables are all radiation-related. When collinearity is reduced, variables are often discarded based on some threshold of the correlation coefficient, e.g. 0.7, but at this point we opt to leave the radiation-related variables be, and reduce collinearity in different ways for the models where it may present problems.

# Classifier

## Description of considered machine learning approaches
In order to use a model that describes the process that creates the data the best, we attempted to use several different kinds of models to assess the data. We test these models on the original dataset or on a suitably modified version of it. After defining each of these models, we would assess which of the models we used performed best and use said model in our predictions.

Specifically, the two measures we used to assess the quality of each model were its accuracy and perplexity. We defined accuracy similarly as in the course material [@book, p. 37] as simply the relative amount of values the model predicted correctly, expressed mathematically for variables of interest $y_i$ and related predictions $\hat{y}_i$ as $\frac{\sum_{i=1}^{n}I(y_i=\hat{y}_i)}{n}$. This measures whether the model defines the labels correctly. All values naturally receieve values within interval $[0,1]$ and higher values imply better results.

Perplexity instead expresses the relative confidence in those predictions. The models we use give estimated probabilities $\hat{p}(\hat{y_i}=y)$ that the datapoints $\hat{y_i}$ we're predicting receive specific values $y$. Thus, if the model is descriptive of the phenomena which create the dataset, it would seem reasonable that for the actual realized values $y$ the measure $\hat{p}(\hat{y_i}=y)$ would be relatively high. Thus its logarithmic transformation $\log(\hat{p}(\hat{y_i}=y))$ would also be able to receive high values, and similarly $-\log(\hat{p}(\hat{y_i}=y))$ and $e^{-\log(\hat{p}(\hat{y_i}=y))}$ would receive small values. Thus we can use as a corresponding overall measure for the dataset the measurement $e^{-\frac{\sum_{i=1}^n\log(\hat{p}(\hat{y_i}=y_{i}))}{n}}$ where $y_i$ are realized values of the observations we're attempting to assess through $\hat{y_i}$. As $\hat{p}(y_i=y)\in [0,1]$ for all $\log(\hat{p}(y_i=y)) \le 0$ for all y. Thus, all values of our general perplexity measure are within interval $[1,\infty)$ and smaller values are preferred.

However, it is not sufficient to simply define the model on the training dataset and assess accuracy and perplexity on said training dataset as this would woefully inflate the accuracy assessment and deflate the perplexity assessment of the model on a general dataset. We instead attempted to assess accuracy and perplexity through cross-validation methods, specifically leave-one-out cross-validation (loo-cv). The idea behind loo-cv according to the course material [@book, p. 200-202] is that for each observation in the training dataset, a model is trained based on all other observations in the dataset. This new model is then used to predict the training dataset value that was left out of defining the model. With this method, we receive based on observations of explanatory variables both predicted values and estimates of probabilities for specific events for each measurement in a way that didn't include the observations in both defining the model training and assessing its quality. Thus, we can use these measures in our accuracy and perplexity calculations to better assess how the model using all the observations in the training dataset behaves on a completely separate test dataset.
```{r preprocess data, echo=FALSE}
# Sample half of the data for training and testing sets
set.seed(42)
idx <- sample(nrow(npfData), nrow(npfData)/2)
npf_train <- npfData[idx, ]
npf_test <- npfData[-idx, ]
```
Next, let's discuss some of the models we considered

### Dummy model

As our first model, we simply attempted to predict the values through a dummy model, which predicts all values into the more common of the two classes. However, as half of the observations are nonevent days and half of the observations are event days, accuracy would only be 50% no matter which the model would choose. As a result, the dummy model is not sufficient to use in this problem

### Logistic regression

According to the course material [@book, p. 133-137], Logistic regression models can be used for binomial variables as well as for multinomial variables under specific assumptions. The idea of logistic regression in a binary case is that our variable of interest $Y_i$ are binomially distributed with parameter $q\in [0,1]$ representing probability that $Y_i$ receieves one of the values. Now, with explanatory variables $X_{ij}$ we assume that these $X_{ij}$ influence the parameter $p_i$ for each observation, meaning $p(Y_i|X_i)=q(X_i)^{Y_i}(1-q(X_i))^{1-Y_i}$ is a binomial probability with parameter $q(X_i)$ being of form $\sigma(\sum_j \lambda_j X_{ij})$ for values of individual explanatory variables $X_{ij}$ and coefficients $\lambda_j$ and link function $\sigma: \mathbb{R}\to [0,1]$. Thus we can estimate a logistic regression by finding parameters $\lambda_j$ that maximize the likelihood function $\prod p(Y_i|X_i)$.

Notable benefits of the logistic regression model include that according to the article "On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes"[@ng2001] as a discriminative model it eventually reaches a lower asymptotic variance than comparable generative models. On top of this, according to the course material [book, p. 387], compared to for example comparable support vector classifiers, on average logistic regression models fare better under situations where the classes are not easily separable by explanatory variables.

However, possible shortcomings of the logistic regression model include assumptions of linearity in relation to the explanatory variables and the parameter (which we could attempt to correct but with such a large collection of explanatory variables, this at least can't be applied immediately), assumptions of for example link function that are required to define coefficients, and especially with high-dimensional models, multicollinearity can become a notable problem.

In our model, we used specifically the logistic link function, implying $\sigma(x)=\frac{e^{x}}{e^{x}+1}$, although we acknowledge that other choices such as the probit link could have also been viable options. The estimation of the model was done with base r function glm. In order to curb issues related to multicollinearity, we first limited ourselves to the dataset with observations measured at multiples only observed at 16.8 meters. After this, we reduced the set of explanatory variables further via variance inflation factors.

According to the course material [@book, p. 102], Variance inflation factors (VIFs) can be calculated for each explanatory variable in a model through assessing linear models for each of the explanatory variables. The VIF for explanatory variable $X_j$ is defined as $\frac{1}{1-R_i^2}$ where $R_i^2$ is the amount of variance of $X_j$ explained through a linear model by the other explanatory variables. Thus, these values express how much the explanatory variable in question can be described by a linear combination of the other explanatory variables. Thus, particularly high VIFs imply multicollinearity being present in the model. We calculated VIFs for our model with function vif from R package car.

As removing one explanatory variable effects the VIFs of all other variables, we chose to implement an iterative process to remove variables based on VIFs. After defining a logistic regression model with all explanatory variables, we calculated the VIFs, assessed which variable had the highest VIF, removed both this variable and the other variable describing the same set of observations (if a mean variable had the highest VIF, we would also remove the corresponding standard deviation and if a standard deviation variable had the highest VIF, we would also remove the corresponding mean variable). We decided to do this as it felt imprudent to choose to not include one descriptor of an observation in the baseline model which we're assessing while including the other one. After this, we would repeat the process until the highest VIF value for a variable was less than 10. The choice of the limit was based on descriptions in the course material.

After this, we had a remaining set of 18 explanatory variables, which we modelled using logistic regression. After checking the coefficients, there were still variables for which the Wald tests gave relatively high p-values for hypotheses involving non-zero coefficients. In order to deal with this, we set those coefficients equal to zero and assessed the model with the remaining explanatory variables. Through this, we received a simpler model with  explanatory variables.

### Lasso regression

Lasso regression has obvious links to logistic regression but it includes a penalty term which punishes models with high values of coefficients. According to the course material [@book, p. 241-242] in comparable linear regression minimizing squared error, the minimizable function is the sum of the squared error terms and $\sum_{i=1}^n \gamma |\lambda_i|$ where $\gamma>0$. We similarly as in logistic regression assess coefficients for a model that describes parameter $q$ of a binomial model that minimizes this function.

One of the benefits of Lasso regression according to the High-Dimensional Statistics course material [@Lasso] is that including this penalty term in the optimization process with correlated variables leads to simpler models with fewer explanatory variables, as also lower-dimensional models lead are preferred. Thus, the lasso model in and of itself can remove variables which are highly correlated with other variables from the model and thus at least alleviate multicollinarity within the model. However, according to the High-Dimensional Statistics course material [@Lasso], selection between possible correlated variables might with extremely highly correlated variables break down due to there being multiple optima. Thus, it seems reasonable to note that with as highly correlated variables as the ones we're dealing with, the model might not properly describe the data-generating process but instead simply choose the variables that happen to create a slightly fit better on the training data.

In terms of our modelling, we decided to use Lasso instead of a comparable method called ridge regression, which according to the course material [@book, p. 237-238] uses penalty term $\sum_{j=1}^m \gamma \lambda_j^2$. The reason for this is that we preferred to make a simpler model with fewer explanatory variables rather than ridge regression, which tends to not reduce the dimension of the model but rather simply give similar coefficients to all correlated explanatory variables. In order to estimate the model, we used R functions glmnet and cv.glmnet from package glmnet [@glmnet]. We attempted to assess the value of $\gamma$ which would minimize loo cross-validated misclassification rate with the function cv.glmnet. After defining this value, we calculated cross-validated accuracy (which should be equivalent to what was calculated by cv.glmnet) as well as the cross-validated perplexity for the model estimated by glmnet.

### Generative models

In logistic regression models, we directly model $p(Y|X)$, i.e. the conditional distribution of the response $Y$ given the predictors $X$. Generative models are covered in [@book, p.141-158]. Generative models offer a less direct approach, where the focus is in modeling $p(X|Y)$ and using these estimates to estimate $p(Y|X)$ for each possible class $k$ with Bayes' theorem:
\begin{equation*}
p(Y=k|X) = \frac{\pi_k p(X|Y=k)}{\sum_i \pi_i p(X|Y=i)}.
\end{equation*}
Here $pi_k$ is an estimate of the prior probability that a random observation comes from the $k$th class, in our case computed as the fraction of the observations in the training dataset that are in the $k$th class. 

While the general approach is the same for all generative models, they differ in how they estimate $p(X|Y)$. In a dataset with $p$ predictors, estimating $p(X|Y)$ amounts to estimating a $p$-dimensional density function for an observation in the $k$th class. The task is challenging, as we must consider the distribution of each predictor on its own *and* the joint distribution of the predictors. Different models make different assumptions that mitigate the difficulty.

#### Linear Discriminant Analysis

A linear discriminant analysis (LDA) classifier assumes that all $p$ predictors $X = (X_1, \dots, X_p)$ are drawn from a multivariate Gaussian distribution. This means that each predictor follows a normal distribution $N(\mu_k, \Sigma)$ where $\mu_k$ is the class-specific mean and $\operatorname{Cov}(X) = \Sigma$ the covariance matrix of $X$, and there is some correlation between each pair of predictors. The estimate of $p(X|Y)$ is
\begin{equation*}
p(X|Y) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \right)
\end{equation*}
meaning that the classifier assigns an observation $X$ to to the class for which
\begin{equation*}
\delta_k(x) = x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
\end{equation*}
is maximized. 
The covariance matrix being the same for all classes is a defining feature of linear discriminant analysis. The assumption makes the model linear and reduces the number of parameters to estimate. This reduces the flexibility and hence lowers the variance of the model. Lower variance can mean that the model performs well, but if the assumption is not reflected in the data, bias can be high and overall performance suffers due to the bias-variance trade-off.

For linear discriminant analysis to work, it is important to have enough observations with regards to the number of predictors, as performance suffers greatly as the number of predictors approaches the number of observations.

In our modeling, we used the function lda from the package [@MASS]. We tested the performance of the model on the dataset with observations measured at multiples only observed at 16.8 meters.

#### Quadratic Disriminant Analysis 

The assumption that the covariance matrix is shared for all classes is quite strict. Allowing each class to have it's own covariance matrix brings us to quadratic discriminant analysis (QDA). The observations are still assumed to be drawn from a multivariate Gaussian distrbution with a class-specific mean vector, so otherwise the assumptions remain the same as in LDA. Each predictor now follows a normal distribution $N(\mu_k, \Sigma_k)$. The expression to be maximized is 
\begin{align*}
\delta_k(x) = x^T\Sigma_k^{-1}\mu_k-\frac{1}{2}x^T\Sigma_k^{-1}x-\frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k -\frac{1}{2} \log |\Sigma_k| + \log \pi_k.
\end{align*}
Generally, quadratic discriminant analysis is expected to perform better that linear discriminant analysis on large data sets, but as in linear discriminant analysis, the number of predictors must be small enough to produce decent results. Quadratic discriminant analysis is also recommended when the assumption that all classes share a covariance matrix is clearly unfounded.

In our approach, we used the qda function from the [@MASS] package, and as with linear discriminant analysis, tested the performance of the model on the dataset with observations measured at multiples only observed at 16.8 meters.

#### Naive Bayes

A naive Bayes classifier makes no assumptions on the distribution of the observations, and instead assumes that within the *k*th class, the *p* predictors are independent. In other words, the assumption is that there is no association between the predictors, and therefore no joint distribution to consider. With this, the posterior probability can be computed as
\begin{equation*}
p(Y=k|X) = \frac{\pi_k p(X_1|Y=k) \cdots p(X_p|Y=k)}{\sum_i \pi_i p(X_1|Y=i) \cdots p(X_p|Y=i)}.
\end{equation*}
In a naive Bayes model, there are a few possible ways to estimate $p(X|Y)$. For quantitative predictors such as ours, we assume that within each class, the *j*th predictor is drawn from a univariate normal distribution, i.e. $X_j|Y = k \approx N (\mu_{jk}, \sigma^2_jk)$.
The assumption of independence between predictors is often not realistic, but the model can perform well nonetheless. This is especially true for small datasets, since estimating a joint distribution requires a large amount of data. 
We used the naiveBayes function from the package [@e1071] and the original dataset.


### Support Vector Classifiers

The idea of the binary support vector classifier is according to the course material [@book, p. 369-370] based on the notion of finding within a $p$-dimensional problem a $(p-1)$-dimensional hyperplane $\lambda_0+\sum\lambda_i x_i=0$ to divide the set of explanatory variables. Now, for this hyperplane, we can divide our observations into two groups, those values for which $\lambda_0+\sum_{i=1}^m\lambda_j x_i>0$ and those values for which $\lambda_0+\sum\lambda_i x_i<0$. Thus, this can function as a model through which we can predict binary variables such as our variable of interest.

However, defining such hyperplanes can be slightly difficult as all values might not be separable by a hyperplane or the hyperplane that separates values can overfit to training data. Thus, according to the course material [@book, p. 373-375], we can define a soft-margin classifier, which has the benefits of letting some values be misclassified or be closer to the hyperplane than a strict maximal margin classifier would define, which leads to a more robust model that fits most of the training dataset better. 

In order to define this, we need to find the optimal hyperplane to separate the observations. According to the course material [@book, p. 375-378], this optimization problem is an optimization problem of $M$ in relation to coefficients $\lambda_i$ and slack variables $\epsilon$ within inequation $y_i(\lambda_0+\sum_{j=1}^m\lambda_jx_{ij})\ge M(1-\epsilon_i)$ with limitations $\sum_{j=1}^{m}\lambda_j^2=1$, $\epsilon_i\ge0$ and $\sum_{i=1}^n\epsilon_i\le C$ for tuning parameter $C$.

According to the course material [@book, p. 380-383], our optimization the data only depends on the inner products of the explanatory variables and the values of the variable of interest. Thus we can use different generalizations of inner products to gain different kinds of models. Because of this, with different choices of kernels we can model different kinds of data.

Other benefits benefits of using support vector classifiers according to the course material [@book, p. 367, 387] include that they have been proven to behave well in several different applications and that it has been shown that on average support vector classifiers perform better than logistic regression models in problems where the classes are well-separated. However, the model can vary quite notably based on the tuning parameter $C$ as this parameter affects the amount of observations that violate the margin and thus increases or decreases the amount of support vectors that influence the model.

For our model, we used functions tune and svm from R package e1071 [@e1071]. We chose to run a 10-fold cross validation on a set of tuning parameters $C$ to assess which kernel performed best and chose to model the value based on that. In these preliminary assessments, we assessed the radial and linear kernels had the best performance of the ones available in package e1071. For these kernels, we ran loo-cross validation to assess the value of tuning parameter $C$ and calculated for said tuning parameter loo-cross-validated accuracy and perplexity. Eventually, the radial kernel had slightly better accuracy but the linear kernel had lower perplexity and as a result, we chose to use the linear kernel in our model.

### Classification trees

In tree-based methods, the data points are divided into smaller and smaller fragments based on some set of rules. A new division happens at each branch, so when the size of the tree grows, it becomes more and more fitted to the data. When fitting classification trees, the process is often first started with a very large tree, and then the unnecessary branches are removed in the process of pruning, until the complexity of the tree is reduces to the optimal level. Constructing the trees, pruning, and evaluating the results can be done in a variety of different ways.

A often used tree-based classification method is called Random Forest. It is based on the process described above, but only a random subset of the features is considered at each split of the tree. This is repeated multiple times and in the end, the obtained forest is evaluated and the best splits at each point are found. For Random Forest classifier, we used the R library *randomForest*.


## Classifier performances (numerical)

In order to reduce knitting time for the file, we will in this report instead of expressing the full R code to calculate the values, simply report the results we received. The results for the two-class models in terms of loo-cross-validated accuracy and perplexity are:
\begin{table}
\begin{tabular}{|r|cc|}
\hline
Model type & loo-cv accuracy & loo-cv perplexity\\
\hline
Dummy model & 0 & 2.01\\
Logistic regression & 0.89 & 1.30\\
Naive Bayes & 0.81 & 152.85\\
LDA & 0.89 & 1.33\\
QDA & 0.85 & 6.34\\
SVM & 0.88 & 1.34\\
Lasso & 0.89 & 1.29\\
Random Forest & 0.89 & 1.35\\
\hline
\end{tabular}
\end{table}

## Chosen classifier, pros and cons of this particular classifier for this application

With our observed best-performing models being the logistic regression model, the random forest model, the Lasso regression model and the linear discriminant analysis. Out of these models, we eventually chose to use the logistic regression model. The Lasso model was our best-performing model, but we eventually chose not to use it for practical reasons related to computational heaviness in calculating the multi-class model. Mainly, the amount of information we could gain through calculations in R without R crashing was relatively limited, to the degree that we couldn't always even calculate the cross-validated best choice for the tuning parameter. We could consider running the models again with a smaller amount of folds in the cross-validation, as that might according to the course material [@book] give more accurate estimates, and thus the model would be runnable. However, for the moment we preferred our current approach as with Lasso we couldn't choose which variables to use in the model, which might not be preferrable in further analyses.

We eventually also chose to not use the LDA model after assessing the assumptions related to the model. LDA models have been found to perform best with a limited amount of variables and we still question whether we should attempt to reduce the amount of variables from the model further from the current amount of 38 from the model. On top of this, considering the assumption of equal variances, the boxplots we drew earlier in the report imply that class "nonevent" might have larger variance for some variables than class "event".

We made the decision of model based on preliminary results, and though classification trees would represent an equally viable solution, we calculated those results only later on, and by that point had already calculated the results for the logistic regression model. However, for the final report we might consider transitioning to the classification tree model.

Thus we were left with the logistic regression model, which was also lower-dimensional than many of the other models with only 10 explanatory variables. However, we could question in relation to this model whether there is sufficient basis to for example remove certain variables, which might describe relevant information, as well as for example whether including an intercept in the model would be meaningful in terms of interpretation. Currently, several of these decisions are based on Wald tests performed within the R [@RDCT2008] function glm, which was pursued in an attempt to create a lower-dimensional model.

## Multiclass-classifier
We will extend the logistic regression model into a multinomial logistic regression model in order to assess the multi-class accuracy for our model. According to the course material [@book, p. 140-141], the basic idea of multinomial logistic regression here is that out of $K$ classes we choose a baseline value of our variable of interest, and compare other values to it, leading to $p(Y=k|X=x)=\frac{e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}{1+\sum_{k=1}^{K-1} e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}$ where our baseline class is class $K$. Alternatively, we can use equivalent so-called softmax coding where we treat all classes equivalently and thus $p(Y=k|X=x)=\frac{e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}{\sum_{k=1}^K-1 e^{\beta_{k0}+\sum_{j=1}^m\beta_{kj}x_{ij}}}$. The interpretation of logistic regression coefficients comes now from the following: $\log(\frac{p(\hat{y}_i=k_a)}{p(\hat{y_i}=k_b)})=(\lambda_{a0}-\lambda_{b0})+\sum_{j=1}^n(\lambda_{aj}-\lambda_{bj})x_{ij}$. Thus the interpretation of these coefficients is in relation to the difference of the coefficients for different classes and describe the ratio between the probabilities that our variable of interest receives a value in one of these two classes.

Similarly as in logistic regression, we attempt to define coefficients $\lambda_j$ in a way that maximizes likelihood.

In our model, we used the same dataset as in our binary logistic regression model, in order to avoid issues of multicollinearity. In order to define this model, we used function multinom from R package nnet [@nnet] with softmax coding. This creates estimated probabilities $\hat{p}(\hat{y}_i=y)$ for all four classes $y$, and through that we could calculate loo-cross-validated values for accuracy as well as perplexity.

Eventually, we chose to emphasize class2 accuracy, and as a result our final predictions are based on estimating models for both class2 and class4. We first used our model for class2 to predict whether a value would be an event or not and for values the model assessing class2 predicted to be event-days, we referred to our class4 model to estimate which of the event classes had the highest probability.

# Results

We eventually have a logistic regression model with 10 explanatory variables which we assume describe the probability of a specific day being an event day. The coefficients of the model are:

\begin{table}
\begin{tabular}{|r|c|}
\hline
Explanatory variable & Estimate of coefficient\\
\hline
CO2168.mean&-0.08822672\\
CO2168.std&0.16589906\\
H20168.mean&-0.75608185\\
NO168.mean&-6.45258751\\
NO168.std&4.78466706\\
Pamb0.std&0.52939712\\
RHIRGA168.mean&-0.07244430\\
RHIRGA168.std&0.19476210\\
SWS.mean&0.04494533\\
SWS.std&0.01404611\\
\hline
\end{tabular}
\end{table}

There are a few things of note about this model. First, it does not have an intercept, which implies that if all observations are at zero, the probability of an event would be 0.5. The validity of this assumption should be studied further in relation to the relevant qualities of NPF events. Second, we notice that we have especially high absolute values of coefficients for the variables related to the variables related to nitrous oxide, though said coefficients also have high standard errots. On top of this, we notice that mostly variables describing means have negative coefficients whereas variables describing standard deviations have positive coefficients. One possible interpretation could be that though lower values of the measurements in question throughout the day increase event probability, simply measuring the mean might not be sufficient, and instead questions of the variations throughout the day are relevant as well, specifically the positive coefficients imply that higher variation or for example higher extreme values within the day increase event probability.

In terms of accuracy, let's observe results by class and see what types of events our model recognizes well:

\begin{table}
\begin{tabular}{|r|cc|}
\hline
Used model&Observations in class&Accuracy of predictions for event-type\\
\hline
Binary model&Event&0.887931\\
Binary model&Nonevent&0.8922414\\
Multinomial model&Ia&0.05882353\\
Multinomial model&Ib&0.4235294\\
Multinomial model&II&0.5840708\\
\hline
\end{tabular}
\end{table}

In terms of the binary model, we have rather similar accuracies for both event days and nonevent days. However, we also notice in terms of multiclass accuracy that while the model predicts in particular nonevents rather well, the LOO-cross-validated predictions are less successful for calculating distinct event types. For example, less than 10% of days which belonged to class Ia were predicted into said class. As a result, further changes should be made to the model in the future to improve these predictions.

## Insights, conclusions, discussion etc.

In order to improve the model, we could ask several different questions concerning the approach we wound up with. First, in terms of variable selection, the measure of standard deviations per day within variables feels somewhat questionable in two different ways: first, it seems relevant to ask what the standard deviation expresses about the phenomena themselves, and whether different kinds of measurements could capture the relevant expressive ideas better, or whether these relevant expressive ideas even exist. Second, since we're assuming in our logisitic regression model that the relationship is a linear combination or at least a linear combination of polynomials of the explanatory variables is relevant: why standard deviation and not variance? For this report, as we don't know sufficienty about the NPF event phenomenon itself, we will simply assume that the standard deviation variables are reasonable within our model and that this scaling is reasonable.

Second, in terms of our variable selection process through VIF and Wald tests, said process is rather simple and could be improved. The VIF iterations also remove corresponding mean and standard deviation variables even if their VIFs are smaller. This was done as the goal of the VIF process was to reduce the amount of phenomena to be observed to as small a collection as seems reasonable, and thus the notion of removing only one of the two variables related to the phenomenon felt slightly odd. Then again, this process was not followed with the Wald tests, as the used interpretation of the Wald tests is specifically more about coefficient values than about the optimization issues caused by multicolinearity, which we chose to discuss the VIF results as. The validity of this approach especially in terms of the VIF process should be questioned, even though results where we run VIF iteratively and only remove the variable that has the highest VIF value gives us a fairly similar model (overall 11 coefficients and an intercept with mostly the same set of explanatory variables).

Also, other variable selection processes such as Lasso (with less computationally expensive cross-validation) and PCA could be used equivalently. However, for the current report, we chose to use our current approach as our chosen method of assessing performance was LOO-CV and thus Lasso was computationally expensive for the multinomial model. Then again, a lower amount of folds in cross-validation could lead to better assessments of the accuracies, and thus be preferrable. We could have also used PCA but by using PCA, we'd lose the interpretability of the coefficients and when running the LOO-cv comparisons, we didn't see notable benefits in terms of accuracy or perplexity from using PCA on a larger collection of variables compared to our current approach of selecting a sufficiently small amount of explanatory variables.

\newpage 

# Self-grading

## Delivarables

**For the deliverables, we give a grade x.**

For the most part, we are pleased with and proud of our project. We took great care in learning about the general theory of machine learning and different classifiers and their implementations. We prepared all our deliverables according to the instructions, and to the best of our ability. We specifically took the time  to write a clear and high-quality report on our approach.

There are some specific issues that we're not completely satisfied with in retrospect. The first being the preprocessing of the data, and the second being the final accuracy and the details surrounding that.

A main contributor to the first problem was probably our work order: we wanted to preprocess the data, and then train and test the models. However, the course had not covered preprocessing while we were working on it, and although we consulted the course book liberally, our understanding and therefore the end result ended up being somewhat lackluster and disorganized.

For example, to reduce multicollinearity, we discarded most of the measurements at different heights because of the high correlation coefficients, and for many models this resulted in a higher accuracy and lower perplexity than what we achieved with the original dataset. Great! On the other hand, we left in the highly correlated radiation-related variables without much discussion or explanation. In short, we started with a preprocessing approach but did not see it to the end.

(Sara: muita preprocessing-probleemoja tai vastaavia, joita haluatte nostaa)

The second problem is perhaps not as grave as the first. Especially in our challenge submission, our accuracy (0.84 for two classes and 0.68 for four) and perplexity (1.47) were not all that great. We agreed before submitting the results that we are not aiming to win anything, and the submission itself is enough. We stand by this. We still could have done better in predicting our own results, and we set to rectify this in our final report. 

Towards the 11th of December, we also found that many of our tree-based methods perform as well or better than logistic regression. If we had found this out earlier, we might have opted for a tree-based method as our final classifier. At that point we did not want to discard the work we had done so far, and opted to try to improve our logistic regression model.


## Group as a whole

**For the group, we give a grade 5.**

We met very soon after being assigned to a group, and started planning and dividing work immediately. We agreed to use several different communication and project management tools, and while working settled on Telegram and weekly face-to-face meetings for communication, Excel for task management and storing key results, and Github for version control. We knew from the start that one member of the group would be absent during the week of the presentations, and planned accordingly.

In our group we had three majors represented: computer science, statistics and mathematics. We all had prior (but differing levels of) experience with R, and not much experience with machine learning. We were able to use our individual experiences to our advantage both in our discussions and while working on our R implementations and the report: we could clarify some of the more difficult concepts together, help each other with R troubleshooting, and read and comment on each other's work.

It was easy for us to trust each other's opinions and implementations, and trust that the agreed upon work gets done well and on time. At times, there were some lulls in the workflow and frustration because of R difficulties, but we were able to solve these situations quickly. We also felt similarly about our deliverables in the end: excellent in some ways, but lacking in others. We were able to have some very constructive discussions about this that highlighted the good atmosphere in the group, and how much we learned during the course to be able to spot the shortfalls in our approach.

\newpage 

# Appendices

## Appendix A

```{r}
summary(npfOrig)
```

