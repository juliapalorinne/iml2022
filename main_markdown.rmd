
---
title: "IML Term project"
author: "Julia Palorinne, Pyry Silomaa, Sara Sippola"
output: pdf_document
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
---

# Set-up and data preprosessing

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
```

```{r load data}
npfOrig <- read.csv("npf_train.csv")
```

### Creating training and testing data
```{r preprocess data, echo=FALSE}
# Create class2
npfOrig$class2[npfOrig$class4 == "nonevent"] <- 0
npfOrig$class2[npfOrig$class4 != "nonevent"] <- 1

# Add date as row name
rownames(npfOrig) <- npfOrig[, "date"]

# Remove columns id, date, class4, partlybad
npfData <- npfOrig[, -(1:4)]

# Sample half of the data for training and testing sets
set.seed(42)
idx <- sample(nrow(npfData), nrow(npfData)/2)
npf_train <- npfData[idx, ]
npf_test <- npfData[-idx, ]
```

### Selecting columns by correlation
```{r correlation plot, echo=FALSE}
# Original correlation s
cmData <- cor(npfData)
corrplot(cmData, order = "FPC", tl.cex = 0.5, tl.col = "black")

# Select global and 16.8 m high values for correlation plot
npf168 <- data.frame(npfOrig[, (5:6)], npfOrig[, (13:16)], npfOrig[, (27:30)], 
                  npfOrig[, (41:42)], npfOrig[, (53:54)], npfOrig[, (63:72)],
                  npfOrig[, (83:88)], npfOrig[, (99:104)])
cm168 <- cor(npf168)
corrplot(cm168, order = "FPC", tl.cex = 0.5, tl.col = "black")
```


```{r functions}

# mean squared error
mse <- function(yhat, y) sqrt(mean((y - yhat)**2))

## accuracy
accuracy <- function(pred, y) mean(ifelse(pred >= 0.5, 1, 0) == y)

## perplexity. Pakotetaan todennäköisyydet tässä välille [0.995, 0.005]
perplexity <- function(pred, y){
  pred <- ifelse(pred != 0 & pred != 1, pred, ifelse(pred == 0, 0.005, 0.995 ))
  exp(-mean(log(ifelse(y == 1, pred, 1 - pred)))) }

# cross-validate
# split n items into k folds of roughly equal size
kpart <- function(n, k) {
    rep_len(1:k, length.out = n)
}

# Find cross-validation predictions
cv <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               n = nrow(data), # number of rows in the data matrix
               k = min(n, 10), # number of cross-validation folds
               split = kpart(n, k), # the split of n data items into k folds
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) predict(model, newdata = data, type = "raw")[,2]) {
    yhat <- NULL
    for (i in 1:k) {
        ## go through all folds, train on other folds, and make a prediction
        mod <- train(data[split != i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[split == i] <- pred(mod, data[split == i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}

# loocv <- function(
#                formula, # Formula specifying which variables to use
#                data, # Dataset
#                model = lm, # Type of model to train (as a function)
#                ## function to train a model on data
#                train = function(data) model(formula, data = data),
#                ## function to make predictions on the trained model
#                pred = function(model, data) predict(model, newdata = data, type = "raw")[,2]) {
#     yhat <- NULL
#     for (i in 1:nrow(data)) {
#         ## go through all stations, train on other stations, and make a prediction
#         mod <- train(data[-i, ])
#         if (is.null(yhat)) {
#             ## initialise yhat to something of correct data type,
#             yhat <- pred(mod, data)
#         } else {
#             yhat[i] <- pred(mod, data[i, ])
#         }
#     }
#     yhat # finally, output cross-validation predictions
# }

```

# Classifiers

## Testing classifiers as they are

### Naive Bayes

```{r naive bayes}

library(e1071)

nb.fit <- naiveBayes(class2 ~ . , data = npf_train, laplace = 1)
nb.pred.cv <- cv(class2 ~ ., npf, naiveBayes) # Hidas prosessi, tehdään siksi vaan kerran

nb_acc <- data.frame(train = accuracy(predict(nb.fit, newdata = npf_train, type = "raw")[,2], npf_train$class2),
                     test = accuracy(predict(nb.fit, newdata = npf_test, type = "raw")[,2], npf_test$class2),
                     CV = accuracy(nb.pred.cv, npf$class2))

# Kaikkiin tulee Inf
nb_perp <- data.frame(train = perplexity(predict(nb.fit, newdata = npf_train, type = "raw")[,2], npf_train$class2),
                      test = perplexity(predict(nb.fit, newdata = npf_test, type = "raw")[,2], npf_test$class2),
                      CV = perplexity(nb.pred.cv, npf$class2))

kable(cbind(nb_acc,
            nb_perp)) %>%
  add_header_above(c("Accuracy" = 3, "Perplexity" = 3))

```

### LDA
```{r lda}

library(MASS)

lda.fit <- lda(class2 ~ . , data = npf_train)

# Cross-validation parametrit
n = nrow(npf) # number of rows in the data matrix
k = min(n, 10) # number of cross-validation folds
split = kpart(n, k) # the split of n data items into k folds
lda.pred.cv <- rep(0,n)

# cv
for (i in 1:k) {
  ## go through all folds, train on other folds, and make a prediction
  mod <- lda(class2 ~ ., npf[split != i, ])
  lda.pred.cv[split == i] <- predict(mod, npf[split == i, ])$posterior[,2]
}


lda_acc <- data.frame(train = accuracy(predict(lda.fit, newdata = npf_train)$posterior[,2], npf_train$class2),
                      test = accuracy(predict(lda.fit, newdata = npf_train)$posterior[,2], npf_test$class2),
                      CV = accuracy(lda.pred.cv, npf$class2))

lda_perp <- data.frame(train = perplexity(predict(lda.fit, newdata = npf_train)$posterior[,2], npf_train$class2),
                      test = perplexity(predict(lda.fit, newdata = npf_train)$posterior[,2], npf_test$class2),
                      CV = perplexity(lda.pred.cv, npf$class2))

kable(cbind(lda_acc,
            lda_perp)) %>%
  add_header_above(c("Accuracy" = 3, "Perplexity" = 3))


```

### QDA
```{r qda}

library(MASS)

qda.fit <- qda(class2 ~ . , data = npf_train)

# Cross-validation parametrit
n = nrow(npf) # number of rows in the data matrix
k = min(n, 10) # number of cross-validation folds
split = kpart(n, k) # the split of n data items into k folds
qda.pred.cv <- rep(0,n)

# cv
for (i in 1:k) {
  ## go through all folds, train on other folds, and make a prediction
  mod <- qda(class2 ~ ., npf[split != i, ])
  qda.pred.cv[split == i] <- predict(mod, npf[split == i, ])$posterior[,2]
}

qda_acc <- data.frame(train = accuracy(predict(qda.fit, newdata = npf_train)$posterior[,2], npf_train$class2),
                      test = accuracy(predict(qda.fit, newdata = npf_train)$posterior[,2], npf_test$class2),
                      CV = accuracy(qda.pred.cv, npf$class2))

# Taas Inf :(
qda_perp <- data.frame(train = perplexity(predict(qda.fit, newdata = npf_train)$posterior[,2], npf_train$class2),
                      test = perplexity(predict(qda.fit, newdata = npf_train)$posterior[,2], npf_test$class2),
                      CV = perplexity(qda.pred.cv, npf$class2))

kable(cbind(qda_acc,
            qda_perp)) %>%
  add_header_above(c("Accuracy" = 3, "Perplexity" = 3))


```

```{r glm}
mod1<-glm(class2~., npf, family=binomial(link="logit"))
mod2<-glm(class2~., npf, family=binomial(link="probit"))
plot(mod1)
prob1<-predict(mod1, npf, type="response")
pred1<-c()
perp1<-c()
for(i in 1:dim(npf)[1]){
  if(prob1[i]>=0.5){
    pred1<-c(pred1,1)
  } else{
    pred1<-c(pred1,0)
  }
  if(npf$class2[i]==1){
    perp1<-c(perp1,prob1[i])
  } else{
    perp1<-c(perp1,1-prob1[i])
  }
}
mean(pred1==npf$class2)
exp(-mean(perp1))
```

```{r lassoridge}
mod3<-cv.glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=1, family="binomial", type.measure="class")
mod4<-cv.glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=0, family="binomial", type.measure = "class")
mod3a<-glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=1, family="binomial", lambda=mod3$lambda.min)
coef(mod3)
plot(mod3a)
plot(mod3$glmnet.fit, xvar="lambda")
prob3a<-predict(mod3a, newx=as.matrix(npf[,1:100]), type="response")
pred3a<-c()
perp3a<-c()
for(i in 1:dim(npf)[1]){
  if(prob3a[i]>=0.5){
    pred3a<-c(pred3a,1)
  } else{
    pred3a<-c(pred3a,0)
  }
  if(npf$class2[i]==1){
    perp3a<-c(perp3a,prob3a[i])
  } else{
    perp3a<-c(perp3a,1-prob3a[i])
  }
}
mean(pred3a==npf$class2)
exp(-mean(perp3a))
mod4a<-glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=0, family="binomial", lambda=mod4$lambda.min)
plot(mod4a)
plot(mod4$glmnet.fit, xvar="lambda")
prob4a<-predict(mod4a, newx=as.matrix(npf[,1:100]), type="response")
pred4a<-c()
perp4a<-c()
for(i in 1:dim(npf)[1]){
  if(prob4a[i]>=0.5){
    pred4a<-c(pred4a,1)
  } else{
    pred4a<-c(pred4a,0)
  }
  if(npf$class2[i]==1){
    perp4a<-c(perp4a,prob4a[i])
  } else{
    perp4a<-c(perp4a,1-prob4a[i])
  }
}
mean(pred4a==npf$class2)
exp(-mean(perp4a))
```
```{r svm}
modsv<-tune(svm, class2~., data=npf, kernel="linear", scale=FALSE, ranges=list(cost=c(0.001, 0.01, 0.1, 1, 2, 5)))
modsvm<-svm(class2~., data=npf, kernel="linear", scale=FALSE, cost=0.001,probability=TRUE)
sv_best<-modsv$best.model
predsv<-predict(modsvm,newdata=npf)
predsv2<-predict(modsvm,newdata=npf,probability = T)
probsv<-attr(predsv2, "probabilities")
perpsv<-c()
for(i in 1:dim(npf)[1]){
  if(npf$class2[i]==0){
    perpsv<-c(perpsv, probsv[i,2])
  } else{
    perpsv<-c(perpsv, probsv[i,1])
  }
  }
mean(predsv==npf$class2)
exp(-mean(log(perpsv)))
```

# First level title
## Second level title
### etc.
