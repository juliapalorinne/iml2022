
---
title: "IML Term project"
author: "Julia Palorinne, Pyry Silomaa, Sara Sippola"
output: pdf_document
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
---

# Set-up and data preprosessing

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load data}
npfOrig <- read.csv("npf_train.csv")
```

Columns selected by correlation

```{r preprocess data}
# Create class2
npfOrig$class2[npfOrig$class4 == "nonevent"] <- 0
npfOrig$class2[npfOrig$class4 != "nonevent"] <- 1

# Add date as row name
rownames(npfOrig) <- npfOrig[, "date"]

# Select global and 16.8 m high values
npf <- data.frame(npfOrig[, (5:6)], npfOrig[, (13:16)], npfOrig[, (27:30)], 
                  npfOrig[, (41:42)], npfOrig[, (53:54)], npfOrig[, (63:72)],
                  npfOrig[, (83:88)], npfOrig[, (99:104)])

# Calculate the correlation matrix
cm <- cor(npf)

# Order variables by 1st principal component
corrplot(cm, order = "FPC", tl.cex = 0.5, tl.col = "black")

```

```{r functions}

# mean squared error
mse <- function(yhat, y) sqrt(mean((y - yhat)**2))

# classification accuracy
cacc <- function(pred, true) sum(as.integer(as.logical(pred == true)))/length(pred)

# cross-validate
# split n items into k folds of roughly equal size
kpart <- function(n, k) {
    rep_len(1:k, length.out = n)
}

# Find cross-validation predictions
cv <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               n = nrow(data), # number of rows in the data matrix
               k = min(n, 20), # number of cross-validation folds
               split = kpart(n, k), # the split of n data items into k folds
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) predict(model, newdata = data)) {
    yhat <- NULL
    for (i in 1:k) {
        ## go through all folds, train on other folds, and make a prediction
        mod <- train(data[split != i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[split == i] <- pred(mod, data[split == i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}

loocv <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) predict(model, newdata = data)) {
    yhat <- NULL
    for (i in 1:nrow(data)) {
        ## go through all stations, train on other stations, and make a prediction
        mod <- train(data[-i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[i] <- pred(mod, data[i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}

```

# Classifiers

## Testing classifiers as they are

```{r naive bayes}

library(e1071)

idx <- sample(nrow(npf), nrow(npf)/2)
npf_train <- npf[idx, ]
npf_test <- npf[-idx, ]

# nb.fit <- naiveBayes(class2 ~ . , data = npf_train)

a <- data.frame(train = cacc(predict(nb.fit, newdata = npf_train), npf_train$class2),
                test = cacc(predict(nb.fit, newdata = npf_test), npf_test$class2),
                CV = cacc(cv(class2 ~ ., npf_train, naiveBayes), npf_train$class2),
                LOOCV = cacc(loocv(class2 ~ ., npf_train, naiveBayes), npf_train$class2))
a

```

```{r glm}
mod1<-glm(class2~., npf, family=binomial(link="logit"))
mod2<-glm(class2~., npf, family=binomial(link="probit"))
plot(mod1)
prob1<-predict(mod1, npf, type="response")
pred1<-c()
perp1<-c()
for(i in 1:dim(npf)[1]){
  if(prob1[i]>=0.5){
    pred1<-c(pred1,1)
  } else{
    pred1<-c(pred1,0)
  }
  if(npf$class2[i]==1){
    perp1<-c(perp1,prob1[i])
  } else{
    perp1<-c(perp1,1-prob1[i])
  }
}
mean(pred1==npf$class2)
exp(-mean(perp1))
```

```{r lassoridge}
mod3<-cv.glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=1, family="binomial", type.measure="class")
mod4<-cv.glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=0, family="binomial", type.measure = "class")
mod3a<-glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=1, family="binomial", lambda=mod3$lambda.min)
coef(mod3)
plot(mod3a)
plot(mod3$glmnet.fit, xvar="lambda")
prob3a<-predict(mod3a, newx=as.matrix(npf[,1:100]), type="response")
pred3a<-c()
perp3a<-c()
for(i in 1:dim(npf)[1]){
  if(prob3a[i]>=0.5){
    pred3a<-c(pred3a,1)
  } else{
    pred3a<-c(pred3a,0)
  }
  if(npf$class2[i]==1){
    perp3a<-c(perp3a,prob3a[i])
  } else{
    perp3a<-c(perp3a,1-prob3a[i])
  }
}
mean(pred3a==npf$class2)
exp(-mean(perp3a))
mod4a<-glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=0, family="binomial", lambda=mod4$lambda.min)
plot(mod4a)
plot(mod4$glmnet.fit, xvar="lambda")
prob4a<-predict(mod4a, newx=as.matrix(npf[,1:100]), type="response")
pred4a<-c()
perp4a<-c()
for(i in 1:dim(npf)[1]){
  if(prob4a[i]>=0.5){
    pred4a<-c(pred4a,1)
  } else{
    pred4a<-c(pred4a,0)
  }
  if(npf$class2[i]==1){
    perp4a<-c(perp4a,prob4a[i])
  } else{
    perp4a<-c(perp4a,1-prob4a[i])
  }
}
mean(pred4a==npf$class2)
exp(-mean(perp4a))
```
```{r svm}
modsv<-tune(svm, class2~., data=npf, kernel="linear", scale=FALSE, ranges=list(cost=c(0.001, 0.01, 0.1, 1, 2, 5)))
modsvm<-svm(class2~., data=npf, kernel="linear", scale=FALSE, cost=0.001,probability=TRUE)
sv_best<-modsv$best.model
predsv<-predict(modsvm,newdata=npf)
predsv2<-predict(modsvm,newdata=npf,probability = T)
probsv<-attr(predsv2, "probabilities")
perpsv<-c()
for(i in 1:dim(npf)[1]){
  if(npf$class2[i]==0){
    perpsv<-c(perpsv, probsv[i,2])
  } else{
    perpsv<-c(perpsv, probsv[i,1])
  }
  }
mean(predsv==npf$class2)
exp(-mean(log(perpsv)))
```

# First level title
## Second level title
### etc.
